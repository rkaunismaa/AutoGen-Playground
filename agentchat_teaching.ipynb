{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday, January 24, 2024\n",
    "\n",
    "This notebook requires you have an YOUR_ARXIV_API_KEY set in your environment variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_teaching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Teaching\n",
    "\n",
    "AutoGen offers conversable agents powered by LLMs, tools, or humans, which can be used to perform tasks collectively via automated chat. This framework makes it easy to build many advanced applications of LLMs.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "This notebook demonstrates how AutoGen enables a user to teach AI new skills via natural agent interactions, without requiring knowledge of programming language. It is modified based on https://github.com/microsoft/FLAML/blob/evaluation/notebook/research_paper/teaching.ipynb and https://github.com/microsoft/FLAML/blob/evaluation/notebook/research_paper/teaching_recipe_reuse.ipynb.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install \"pyautogen>=0.2.3\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet \"pyautogen>=0.2.3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n",
    "\n",
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well).\n",
    "\n",
    "The json looks like the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"<your OpenAI API key here>\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"<your Azure OpenAI API key here>\",\n",
    "        \"base_url\": \"<your Azure OpenAI API base here>\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-32k\",\n",
    "        \"api_key\": \"<your Azure OpenAI API key here>\",\n",
    "        \"base_url\": \"<your Azure OpenAI API base here>\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "# llm_config = {\n",
    "#     \"timeout\": 600,\n",
    "#     \"cache_seed\": 44,  # change the seed for different trials\n",
    "#     \"config_list\": autogen.config_list_from_json(\n",
    "#         \"OAI_CONFIG_LIST\",\n",
    "#         filter_dict={\"model\": [\"gpt-4-32k\"]},\n",
    "#     ),\n",
    "#     \"temperature\": 0,\n",
    "# }\n",
    "\n",
    "config_list=[\n",
    "    {\n",
    "        \"model\": \"LMStudio Local Model\",\n",
    "        \"base_url\": \"http://localhost:1234/v1\",\n",
    "        \"api_type\": \"open_ai\",\n",
    "        \"api_key\": \"NULL\", # required, you donÂ´t need to change this\n",
    "        }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Task: Literature Survey\n",
    "\n",
    "We consider a scenario where one needs to find research papers of a certain topic, categorize the application domains, and plot a bar chart of the number of papers in each domain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Agents\n",
    "\n",
    "We create an assistant agent to solve tasks with coding and language skills. We create a user proxy agent to describe tasks and execute the code suggested by the assistant agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"work_dir\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "Find arxiv papers that show how are people studying trust calibration in AI based systems\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\n",
      "First, I will use my language skill to search for relevant keywords on ArXiv using a web browser or a Python library like `arxiv-sanity` to find papers related to \"trust calibration in AI based systems\". Once I have the list of papers, I will print their titles and authors for you. After that, I will use Python to download the full text of the selected papers if they are available on ArXiv. Here's the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "# filename: arxiv_search.py\n",
      "import requests\n",
      "import json\n",
      "from arxiv import query\n",
      "\n",
      "def search_papers(query):\n",
      "    api_key = \"YOUR_ARXIV_API_KEY\"\n",
      "    papers = query(api_key, query)\n",
      "    return papers['entries']\n",
      "\n",
      "def download_paper(id, output_file):\n",
      "    url = f\"https://arxiv.org/abs/{id}\"\n",
      "    response = requests.get(url)\n",
      "    with open(output_file, 'w') as file:\n",
      "        file.write(response.text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    query = \"trust calibration AI based systems\"\n",
      "    papers = search_papers(query)\n",
      "    for paper in papers[:5]: # download top 5 papers\n",
      "        title = paper['title']\n",
      "        authors = ', '.join([author['given'] + ' ' + author['family'] for author in paper['authors']])\n",
      "        id = paper['id']\n",
      "        print(f\"Title: {title}\\nAuthors: {authors}\")\n",
      "        output_file = f\"{title.replace(' ', '_').lower()}.pdf\"\n",
      "        download_paper(id, output_file)\n",
      "```\n",
      "\n",
      "Replace `YOUR_ARXIV_API_KEY` with your ArXiv API key. This code will search for papers related to the query and print their titles and authors. It will also download the full text of the top 5 papers as PDF files. After running this script, you should have the required information to read the papers offline.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task1 = \"\"\"\n",
    "Find arxiv papers that show how are people studying trust calibration in AI based systems\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "analyze the above the results to list the application domains studied by these papers \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To analyze the application domains studied in the downloaded papers, I will use Python's Natural Language Toolkit (NLTK) library for text processing and extracting keywords. Here's the code to accomplish this task:\n",
      "\n",
      "```python\n",
      "# filename: analyze_papers.py\n",
      "import json\n",
      "import re\n",
      "import nltk\n",
      "from arxiv import query\n",
      "from nlp import downloader, tokenize, pos_tag, remove_stopwords\n",
      "\n",
      "def search_papers(query):\n",
      "    api_key = \"YOUR_ARXIV_API_KEY\"\n",
      "    papers = query(api_key, query)\n",
      "    return papers['entries']\n",
      "\n",
      "def extract_keywords(text):\n",
      "    downloader.download('punkt')\n",
      "    downloader.download('averaged_perceptron_tagger')\n",
      "    words = tokenize(text)\n",
      "    tagged_words = pos_tag(words)\n",
      "    keywords = []\n",
      "    for word, tag in tagged_words:\n",
      "        if tag[0] in ['NN', 'NNS', 'JJ', 'JJR', 'JJS'] and not re.search(r'\\d+', word): # extract nouns and adjectives\n",
      "            keywords.append(word)\n",
      "    return keywords\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    query = \"trust calibration AI based systems\"\n",
      "    papers = search_papers(query)[:5]\n",
      "    for paper in papers:\n",
      "        id = paper['id']\n",
      "        title = paper['title']\n",
      "        text = paper['abstract'] # extract abstract text\n",
      "        keywords = extract_keywords(text)\n",
      "        print(f\"Title: {title}\\nKeywords: {' '.join(keywords)}\")\n",
      "```\n",
      "\n",
      "Replace `YOUR_ARXIV_API_KEY` with your ArXiv API key. This code will analyze the abstracts of the downloaded papers and extract keywords related to their application domains. After running this script, you should have a list of keywords for each paper indicating the application domain studied in the paper.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task2 = \"analyze the above the results to list the application domains studied by these papers \"\n",
    "user_proxy.initiate_chat(assistant, message=task2, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3 = \"\"\"Use this data to generate a bar chart of domains and number of papers in that domain and save to a file\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task3, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# Image(filename='work_dir/domains_bar_chart.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Recipes\n",
    "\n",
    "Now that the task has finished via a number of interactions. The user does not want to repeat these many steps in future. What can the user do?\n",
    "\n",
    "A followup request can be made to create a reusable recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task4 = \"\"\"Reflect on the sequence and create a recipe containing all the steps\n",
    "necessary and name for it. Suggest well-documented, generalized python function(s)\n",
    " to perform similar tasks for coding steps in future. Make sure coding steps and\n",
    " non-coding steps are never mixed in one function. In the docstr of the function(s),\n",
    " clarify what non-coding steps are needed to use the language skill of the assistant.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task4, clear_history=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse Recipes\n",
    "\n",
    "The user can apply the same recipe to similar tasks in future.\n",
    "\n",
    "### Example Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"work_dir\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "task1 = '''\n",
    "This recipe is available for you to reuse..\n",
    "\n",
    "<begin recipe>\n",
    "**Recipe Name:** Analyzing and Visualizing Application Domains in Arxiv Papers\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Collect relevant papers from arxiv using a search query.\n",
    "2. Analyze the abstracts of the collected papers to identify application domains.\n",
    "3. Count the number of papers in each application domain.\n",
    "4. Generate a bar chart of the application domains and the number of papers in each domain.\n",
    "5. Save the bar chart as an image file.\n",
    "\n",
    "Here are the well-documented, generalized Python functions to perform the coding steps in the future:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def search_arxiv(query: str, max_results: int = 10) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search arxiv for papers related to a specific query.\n",
    "\n",
    "    :param query: The search query for arxiv papers.\n",
    "    :param max_results: The maximum number of results to return. Default is 10.\n",
    "    :return: A list of dictionaries containing the title, link, and summary of each paper.\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = f\"search_query=all:{query}\"\n",
    "    start = 0\n",
    "    max_results = f\"max_results={max_results}\"\n",
    "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
    "    response = requests.get(url)\n",
    "    feed = feedparser.parse(response.content)\n",
    "\n",
    "    papers = [{\"title\": entry.title, \"link\": entry.link, \"summary\": entry.summary} for entry in feed.entries]\n",
    "    return papers\n",
    "\n",
    "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a bar chart of application domains and the number of papers in each domain, and save it as an image file.\n",
    "\n",
    "    :param domains: A dictionary containing application domains as keys and the number of papers as values.\n",
    "    :param output_file: The name of the output image file.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(domains.keys(), domains.values())\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Application Domains\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.title(\"Number of Papers per Application Domain\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "1. Use the `search_arxiv` function to collect relevant papers from arxiv using a search query.\n",
    "2. Analyze the abstracts of the collected papers using your language skills to identify application domains and count the number of papers in each domain.\n",
    "3. Use the `generate_bar_chart` function to generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
    "\n",
    "</end recipe>\n",
    "\n",
    "\n",
    "Here is a new task:\n",
    "Plot a chart for application domains of GPT models\n",
    "'''\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=task1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
